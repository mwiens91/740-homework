% Set up the document
\documentclass{article}

% Page size
\usepackage[
    letterpaper,]{geometry}
\usepackage{changepage}

% Lines between paragraphs
\setlength{\parskip}{\baselineskip}
\setlength{\parindent}{0pt}

% Math
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{commath}

% Shortcut for boldface
\def\*#1{\mathbf{#1}}

% Operators
%\DeclareMathOperator{\rank}{rank}

% Asymptotic notations
\newcommand{\Oh}{\mathcal{O}}

% Number sets
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}

% Links
\usepackage{hyperref}

% Graphics
\usepackage{float}
\usepackage{graphicx}
\graphicspath{ {./img/} }

% Page numbers at top right
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{\thepage}
\renewcommand\headrulewidth{0pt}

\begin{document}

\textbf{AMATH 740 assignment 3} \\
\textbf{Matt Wiens} \\
\textbf{2020-11-27}

1. \textbf{CG step length.}
Consider the CG method for $A \*x = \*b$ with $A$ SPD. Show that, in the update formula
%
\begin{equation*}
    \*x_k = \*x_{k - 1} + \alpha_k \*p_{k - 1}
    ,
\end{equation*}
%
CG chooses the step length that minimizes $\phi(\*x)$ along the along direction $\*p_{k - 1}$,
as in steepest descent:
%
\begin{equation*}
    \dod{}{\alpha_k} \phi(\*x_k(\alpha_k)) = 0.
\end{equation*}

(a) Show first that this requires step length
%
\begin{equation*}
    \alpha_k = \frac{\*r_{k - 1}^T \*p_{k - 1}}{\*p_{k - 1}^T A \*p_{k - 1}}
    .
\end{equation*}
%
\textit{Solution.}
We take the derivative as follows:
%
\begin{equation*}
    \dod{}{\alpha_k} \phi(\*x_k(\alpha_k))
        = \nabla \phi(\*x_k)^T \dod{\*x_k}{\alpha_k}
        = (-\*r_k)^T \*p_{k - 1}
        = - \*r_k^T \*p_{k - 1}
        .
\end{equation*}
%
Then setting this expression to $0$ gives us
%
\begin{align*}
    &- \*r_k^T \*p_{k - 1} = 0 \\
    &\iff \*r_k^T \*p_{k - 1} = 0 \\
    &\iff (\*b - A \*x_k)^T \*p_{k - 1} = 0 \\
    &\iff (\*b - A (\*x_{k - 1} + \alpha_k \*p_{k - 1}))^T \*p_{k - 1} = 0 \\
    &\iff (\*r_{k - 1} - \alpha_k A \*p_{k - 1})^T \*p_{k - 1} = 0 \\
    &\iff \*r_{k - 1}^T \*p_{k - 1} - \alpha_k \*p_{k - 1}^T A^T \*p_{k - 1} = 0 \\
    &\iff \*r_{k - 1}^T \*p_{k - 1} - \alpha_k \*p_{k - 1}^T A \*p_{k - 1} = 0 \\
    &\iff \alpha_k = \frac{\*r_{k - 1}^T \*p_{k - 1}}{\*p_{k - 1}^T A \*p_{k - 1}},
\end{align*}
%
which is our desired result.

\vspace{5mm}

(b) Using properties we showed in class for the CG method, show that this $\alpha_k$
equals the $\alpha_k$ used in the CG algorithm:
%
\begin{equation*}
    \alpha_k = \frac{\*r_{k - 1}^T \*r_{k - 1}}{\*p_{k - 1}^T A \*p_{k - 1}}
    .
\end{equation*}
%
\textit{Solution.}
Recall that the search direction $\*p_k$ is given by $\*p_0 = \*r_0$
and for $k > 0$,
%
\begin{equation*}
    \*p_k = \*r_k + \beta_k \*p_{k - 1}
    .
\end{equation*}
%
We can see that if we ``unravel'' the recursion we will have
%
\begin{equation*}
    \*p_k = \*r_k + \sum_{i = 0}^{k - 1} \prod_{j = i + 1}^k \beta_j \*r_{i}
    .
\end{equation*}
%
We know for the CG algorithm, all residuals are mutually orthogonal. Hence
%
\begin{align*}
    \*r_{k - 1}^T \*p_{k - 1}
    &=
    \*r_{k - 1}^T \del{\*r_{k - 1} + \sum_{i = 0}^{k - 2} \prod_{j = i + 1}^{k - 1} \beta_j \*r_{i}}
    \\
    &=
    \*r_{k - 1}^T \*r_{k - 1} + \sum_{i = 0}^{k - 2} \prod_{j = i + 1}^{k - 1} \beta_j \*r_{k - 1}^T \*r_{i}
    \\
    &=
    \*r_{k - 1}^T \*r_{k - 1} + \sum_{i = 0}^{k - 2} \prod_{j = i + 1}^{k - 1} \beta_j 0
    \\
    &= \*r_{k - 1}^T \*r_{k - 1}
    .
\end{align*}
%
Hence our expression from part (a) becomes
%
\begin{equation*}
    \alpha_k = \frac{\*r_{k - 1}^T \*r_{k - 1}}{\*p_{k - 1}^T A \*p_{k - 1}}
    .
\end{equation*}

\newpage

2. \textbf{High-order ODE.}
Write the following third order ODE as a first-order ODE system:
%
\begin{equation*}
    y^{\prime \prime \prime}(x) + 3 y^{\prime \prime}(x) - 4 y^\prime(x) + 7 y(x) = x^2 + 7,
\end{equation*}
%
and give the system in matrix form.

\textit{Solution.}
Here we set
%
\begin{align*}
    y_1(x) &\coloneqq y(x), \\
    y_2(x) &\coloneqq y^\prime(x), \\
    y_3(x) &\coloneqq y^{\prime \prime} (x),
\end{align*}
%
which gives us the system (where we supress the arguments $x$)
%
\begin{align*}
    y_1^\prime &= y_2, \\
    y_2^\prime &= y_3, \\
    y_3^\prime &= x^2 + 7 - 3 y_3 + 4 y_2 - 7 y_1.
\end{align*}
%
We can write this in matrix form as
%
\begin{equation*}
    \begin{bmatrix}
        y_1 \\
        y_2 \\
        y_3
    \end{bmatrix}^\prime
    =
    \begin{bmatrix}
        0 & 1 & 0 \\
        0 & 0 & 1 \\
        -7 & 4 & -3
    \end{bmatrix}
    \begin{bmatrix}
        y_1 \\
        y_2 \\
        y_3
    \end{bmatrix}
    +
   \begin{bmatrix}
       0 \\
       0 \\
       x^2 + 7
   \end{bmatrix}
   .
\end{equation*}

\newpage

3. \textbf{Local truncation error for Ralston method.}
Consider the Ralson method for ODE $y^\prime = f(x, y)$:
%
\begin{align*}
    k_1 &= f(x_n, y_n), \\
    k_2 &= f\del{x_n + \frac{2}{3} h, y_n + \frac{2}{3}h k_1}, \\
    y_{n + 1} &= y_n + h \del{\frac{1}{4} k_1 + \frac{3}{4} k_2}.
\end{align*}
%
Show that the local truncation error at $x_{n + 1}$, given by
$l_{n + 1} = \hat{y}(x_{n + 1}) - y_{n + 1}$, is $\Oh(h^3)$.
(Note: We assume as usual that $f$ and its derivatives are sufficiently smooth and bounded. The Ralson method is a 2-stage RK method.)

\textit{Solution.}
Here we take an approach similar to the approach taken when showing the local truncation error for the Euler-Cauchy
method in the notes. To start, we have
%
\begin{align*}
    l_{n + 1}
        &= \hat{y}(x_{n + 1}) - y_{n + 1} \\
        &= \del{\hat{y}(x_{n}) + \hat{y}^\prime(x_n)h + \hat{y}^{\prime \prime} \frac{h^2}{2} + \Oh(h^3)}
            \\
        &\quad-
            \del{y_{n} + h \del{\frac{1}{4} f(x_n, y_n) + \frac{3}{4} f\del{x_n + \frac{2}{3} h, y_n + \frac{2}{3}h f(x_n, y_n)}}}
            .
\end{align*}
%
Now, we note the following:
%
\begin{align*}
    \hat{y}(x_n) &= y_n,
    \\
    \hat{y}^\prime(x_n) &= f(x_n, \hat{y}(x_n)) = f(x_n, y_n) \eqqcolon f_n,
    \\
    \hat{y}^{\prime \prime}(x_n)
        &= \dpd{f}{x} (x_n, \hat{y}(x_n))
        + \dpd{f}{y} (x_n, \hat{y}(x_n))
        f(x_n, \hat{y}(x_n))
        \\
        &= \dpd{f_n}{x}
        + \dpd{f_n}{y} f_n
        ,
\end{align*}
%
where in the last line we used that
%
\begin{equation*}
    \hat{y}^{\prime \prime}(x)
        = \dpd{f}{x} + \dpd{f}{y} \dod{y}{x}
        = \dpd{f}{x} + \dpd{f}{y} f
        .
\end{equation*}
%
Thus we have that
%
\begin{equation*}
    \hat{y}(x_{n}) + \hat{y}^\prime(x_n)h + \hat{y}^{\prime \prime} \frac{h^2}{2}
    = y_n + h f_n + \frac{h^2}{2} \del{\dpd{f_n}{x} + \dpd{f_n}{y} f_n}
    .
\end{equation*}
%
Now focusing on the second term in our equation for $l_{n + 1}$, by
using a Taylor expansion, we have that
%
\begin{align*}
    f\del{x_n + \frac{2}{3} h, y_n + \frac{2}{3}h f(x_n, y_n)}
        &= f(x_n, y_n)
            + \dpd{f}{x} (x_n, y_n) \frac{2}{3} h
            + \dpd{f}{y} (x_n, y_n) \frac{2}{3} h f(x_n, y_n)
            + \Oh(h^2)
            \\
        &= f_n
            + \frac{2}{3} h \dpd{f_n}{x}
            + \frac{2}{3} h f_n \dpd{f_n}{y}
            + \Oh(h^2)
            ,
\end{align*}
%
and so
%
\begin{align*}
    &y_{n} + h \del{\frac{1}{4} f(x_n, y_n) + \frac{3}{4} f\del{x_n + \frac{2}{3} h, y_n + \frac{2}{3}h f(x_n, y_n)}} \\
        &= y_n
            + \frac{1}{4} h f_n
            + \frac{3}{4} h \del{
                f_n
                + \frac{2}{3} h \dpd{f_n}{x}
                + \frac{2}{3} h f_n \dpd{f_n}{y}
                + \Oh(h^2)
            }
            \\
        &= y_n
            + \frac{1}{4} h f_n
            + \frac{3}{4} h f_n
            + \frac{1}{2} h^2 \dpd{f_n}{x}
            + \frac{1}{2} h^2 f_n \dpd{f_n}{y}
            + \Oh(h^3)
            \\
        &= y_n + h f_n + \frac{h^2}{2} \del{\dpd{f_n}{x} + \dpd{f_n}{y} f_n} + \Oh(h^3)
        \\
        &= \hat{y}(x_{n}) + \hat{y}^\prime(x_n)h + \hat{y}^{\prime \prime} \frac{h^2}{2} + \Oh(h^3)
        .
\end{align*}
%
Thus we see that in our equation for $l_{n + 1}$ that
%
\begin{equation*}
    l_{n + 1} = \Oh(h^3)
    .
\end{equation*}

\newpage

4. \textbf{Local truncation error for $\boldsymbol\eta$ method}.
Consider numerical method
%
\begin{equation*}
    y_{n + 1} = y_n + h f(x_n + (1 - \eta) h, \eta y_n + (1 - \eta) y_{n + 1}), \quad \eta \in [0, 1]
\end{equation*}
%
for $y^\prime = f(x, y)$. Show that the local truncation error $l_{n + 1} = \Oh(h^2)$ for any $\eta \in [0, 1]$. Is there
a value of $\eta$ for which $l_{n + 1} = \Oh(h^3)$?

(Note: We assume as usual that $f$ and its derivatives are sufficiently smooth and bounded.)

\textit{Solution.}
The approach here is similar to that of the previous question. To start, we have
%
\begin{align*}
    l_{n + 1}
        &= \hat{y}(x_{n + 1}) - y_{n + 1} \\
        &= \del{\hat{y}(x_{n}) + \hat{y}^\prime(x_n)h + \hat{y}^{\prime \prime} \frac{h^2}{2} + \Oh(h^3)}
            - \del{y_{n} + h f(x_n + (1 - \eta) h, \eta y_n + (1 - \eta) y_{n + 1})}
            .
\end{align*}
%
As before, we identify
%
\begin{align*}
    \hat{y}(x_n) &= y_n,
    \\
    \hat{y}^\prime(x_n) &= f_n,
    \\
    \hat{y}^{\prime \prime}(x_n)
        &= \dpd{f_n}{x}
        + \dpd{f_n}{y} f_n
        ,
\end{align*}
%
and so
%
\begin{equation*}
    \hat{y}(x_{n}) + \hat{y}^\prime(x_n)h + \hat{y}^{\prime \prime} \frac{h^2}{2}
    = y_n + h f_n + \frac{h^2}{2} \del{\dpd{f_n}{x} + \dpd{f_n}{y} f_n}
    .
\end{equation*}
%
First, note that from the initial equation we were given, we have
%
\begin{equation*}
    \eta y_n + (1 - \eta) y_{n + 1}
    = \eta y_n + (1 - \eta) (y_n + h C(n, \eta))
    ,
\end{equation*}
%
where
%
\begin{equation*}
    C(n, \eta) = f(x_n + (1 - \eta) h, \eta y_n + (1 - \eta) y_{n + 1})
    .
\end{equation*}
%
Hence we can write
%
\begin{equation*}
    f(x_n + (1 - \eta) h, \eta y_n + (1 - \eta) y_{n + 1})
    = f(x_n + (1 - \eta) h, y_n + h C(n, \eta))
    .
\end{equation*}
%
Now, taking a Taylor series again we have that
%
\begin{align*}
    &f(x_n + (1 - \eta) h, \eta y_n + (1 - \eta) y_{n + 1})
    \\
    &= f(x_n + (1 - \eta) h, y_n + h C(n, \eta))
    \\
    &= f(x_n, y_n)
        + \dpd{f}{x} (x_n, y_n) (1 - \eta) h
        + \dpd{f}{y} (x_n, y_n) h C(n, \eta)
        + \Oh(h^2)
        \\
    &= f_n
        + (1 - \eta) h \dpd{f_n}{x}
        + C(n, \eta) h \dpd{f_n}{y}
        + \Oh(h^2)
    .
\end{align*}
%
Thus we have that for any $\eta$,
%
\begin{align*}
    l_{n + 1}
        &= \hat{y}(x_{n + 1}) - y_{n + 1} \\
        &= \del{\hat{y}(x_{n}) + \hat{y}^\prime(x_n)h + \hat{y}^{\prime \prime} \frac{h^2}{2} + \Oh(h^3)}
            - \del{y_{n} + h f(x_n + (1 - \eta) h, \eta y_n + (1 - \eta) y_{n + 1})}
        \\
        &= \del{y_n + h f_n + \frac{h^2}{2} \del{\dpd{f_n}{x} + \dpd{f_n}{y} f_n} + \Oh(h^3)}
        \\
        &\quad -
            \del{
                y_n
                +
                h
                \del{
                f_n
                + (1 - \eta) h \dpd{f_n}{x}
                + C(n, \eta) h \dpd{f_n}{y}
                + \Oh(h^2)
                }
            }
        \\
        &= h^2 \del{
            \frac{1}{2}
            \del{\dpd{f_n}{x} + \dpd{f_n}{y} f_n}
            - (1 - \eta) \dpd{f_n}{x}
            - C(n, \eta) \dpd{f_n}{y}
        }
        + \Oh(h^3)
        .
\end{align*}
%
Note that clearly $l_{n + 1} = \Oh(h^2)$.
Now if $\eta = 1/2$ then using our definitions and taking a Taylor expansion we have
%
\begin{align*}
    C\del{n, \frac{1}{2}}
        &= f\del{x_n + \frac{1}{2} h, \frac{1}{2} y_n + \frac{1}{2} y_{n + 1}} \\
        &= f\del{x_n + \frac{1}{2} h, y_n + h C\del{n, \frac{1}{2}}} \\
        &= f(x_n, y_n) + \Oh(h) \\
        &= f_n + \Oh(h)
\end{align*}
%
and hence using this $\eta$ we have
%
\begin{align*}
    l_{n + 1}
        &= h^2 \del{
            \frac{1}{2}
            \del{\dpd{f_n}{x} + \dpd{f_n}{y} f_n}
            - \del{1 - \frac{1}{2}} \dpd{f_n}{x}
            - C\del{n, \frac{1}{2}} \dpd{f_n}{y}
        }
        + \Oh(h^3)
        \\
        &= h^2 \del{
            \frac{1}{2} \dpd{f_n}{y} f_n
            - C\del{n, \frac{1}{2}} \dpd{f_n}{y}
        }
        \\
        &= h^2 \del{
            \frac{1}{2} \dpd{f_n}{y} f_n
            - \dpd{f_n}{y} f_n
        }
        + \Oh(h^3)
        \\
        &= -h^2 \frac{1}{2} \dpd{f_n}{y} f_n
        + \Oh(h^3)
        .
\end{align*}
%
Note that for $l_{n + 1}$ to be $\Oh(h^3)$ we would require $\eta = 1/2 + \Oh(h)$,
but we have just shown that for such an $\eta$ (where we omitted the $\Oh(h)$ term
in the calculations above for readability), $l_{n + 1} = \Oh(h^2)$. Hence $l_{n + 1}$
is never $\Oh(h^3)$.

\newpage

5. \textbf{Accurate approximation at $\boldsymbol{x_1}$}.
You are given the ODE
%
\begin{equation*}
    y^\prime(x) = f(x, y)
\end{equation*}
%
with explicit knowledge of $f(x, y)$ as a function of $x$ and $y$. Assume that, given the initial condition $y_0 = y(x_0)$,
you need a starting value for $y_1$ at $x_1 = x_0 + h$ that is accurate with order $h^4$. Find an explicit method for calculating
such an accurate starting value $y_1$, using only evaluations of $f(x, y)$ and its partial derivatives at $x_0$.

\textit{Solution.}
Again using the shorthand notation that
%
\begin{equation*}
    f_n = f(x_n, y_n),
\end{equation*}
%
we have that
%
\begin{align*}
    \hat{y}(x_n) &= y_n,
    \\
    \hat{y}^\prime(x_n) &= f_n,
    \\
    \hat{y}^{\prime \prime}(x_n)
        &= \dpd{f_n}{x}
        + \dpd{f_n}{y} f_n
    \\
    \hat{y}^{\prime \prime \prime}(x_n)
        &= \dpd[2]{f_n}{x}
        + 2 \dmd{f_n}{2}{x}{}{y}{} f_n
        + \dpd[2]{f_n}{y} f_n^2
        + \dpd{f_n}{y} \dpd{f_n}{x}
        + \sbr{\dpd{f_n}{y}}^2 f_n
\end{align*}
%
where we used that
%
\begin{equation*}
    \hat{y}^{\prime \prime}(x)
        = \dpd{f}{x} + \dpd{f}{y} \dod{y}{x}
        = \dpd{f}{x} + \dpd{f}{y} f
\end{equation*}
%
and
%
\begin{align*}
    \hat{y}^{\prime \prime \prime}(x)
    &= \dpd[2]{f}{x}
    + \dmd{f}{2}{x}{}{y}{} \dod{y}{x}
    + \del{
        \dpd[2]{f}{y} \dod{y}{x}
        + \dmd{f}{2}{y}{}{x}{}
    } f
    + \dpd{f}{y} \del{\dpd{f}{x} + \dpd{f}{y} f}
    \\
    &= \dpd[2]{f}{x}
    + \dmd{f}{2}{x}{}{y}{} \dod{y}{x}
    + \dpd[2]{f}{y} \dod{y}{x} f
    + \dmd{f}{2}{y}{}{x}{} f
    + \dpd{f}{y} \dpd{f}{x}
    + \dpd{f}{y} \dpd{f}{y} f
    \\
    &= \dpd[2]{f}{x}
    + 2 \dmd{f}{2}{x}{}{y}{} f
    + \dpd[2]{f}{y} f^2
    + \dpd{f}{y} \dpd{f}{x}
    + \sbr{\dpd{f}{y}}^2 f
    .
\end{align*}
%
Hence if we take our method to be
%
\begin{equation*}
    y_1 =
        y_0
        + h f_0
        + \frac{h^2}{2}
        \del{
            \dpd{f_0}{x}
            + \dpd{f_0}{y} f_0
        }
        + \frac{h^3}{6}
        \del{
            \dpd[2]{f_0}{x}
            + 2 \dmd{f_0}{2}{x}{}{y}{} f_0
            + \dpd[2]{f_0}{y} f_0^2
            + \dpd{f_0}{y} \dpd{f_0}{x}
            + \sbr{\dpd{f_0}{y}}^2 f_0
        }
\end{equation*}
%
Then by construction we will have
%
\begin{equation*}
    l_1 = \Oh(h^4)
    .
\end{equation*}

\newpage

6. \textbf{Numerical stability of backward Euler method}.
Find the region $D$ of absolute stability for the backward Euler method
%
\begin{equation*}
    y_{n + 1} = y_n + h f(x_{n + 1}, y_{n + 1})
\end{equation*}
%
in the complex $h \lambda$ plane.

(Note: this is an implicit one-step method, and can be derived similar to the forward Euler method.)

\textit{Solution.}
For absolute stability we use the test equation as per the course notes to get
%
\begin{align*}
    y_{n + 1}
        &= y_n + h f(x_{n + 1}, y_{n + 1}) \\
        &= y_n + h \lambda y_{n + 1},
\end{align*}
%
and so
%
\begin{align*}
    y_{n + 1}
        &= \frac{1}{1 - h \lambda} y_n \\
        &= \frac{1}{(1 - h \lambda)^n} y_0.
\end{align*}
%
Hence to satisfy $\lim_{n \to \infty} y_n = 0$ we need
%
\begin{align*}
    &\envert{\frac{1}{1 - h \lambda}} < 1
    \\
    &\iff 1 < |1 - h \lambda|
    \\
    &\iff 1 < \sqrt{(1 - \Re(h \lambda))^2 + \Im(- h \lambda)^2}
    \\
    &\iff 1 < \sqrt{(1 - \Re(h \lambda))^2 + \Im(h \lambda)^2}
    .
\end{align*}
%
Hence if we let $C$ be the set $C = \{|z - 1| \leq 1\}$ then we have that $D = \C \setminus C$.

\newpage

7. \textbf{Numerical stability of Ralston method.}
Consider Ralson's method
%
\begin{align*}
    k_1 &= f(x_n, y_n), \\
    k_2 &= f\del{x_n + \frac{2}{3} h, y_n + \frac{2}{3}h k_1}, \\
    y_{n + 1} &= y_n + h \del{\frac{1}{4} k_1 + \frac{3}{4} k_2}.
\end{align*}
%
(a) Show that the region $D$ of absolute stability for this method in the complex $h \lambda$ plane is
described by the condition
%
\begin{equation*}
    |1 + h \lambda + h^2 \lambda^2 / 2| < 1.
\end{equation*}
%
\textit{Solution.}
Using the same test equation as the notes, we have that
%
\begin{equation*}
    k_1 = f(x_n, y_n) = \lambda y_n
    .
\end{equation*}
%
Then we have that
%
\begin{align*}
    k_2
    &= f\del{x_n + \frac{2}{3} h, y_n + \frac{2}{3}h k_1}
    \\
    &= f\del{x_n + \frac{2}{3} h, y_n + \frac{2}{3}h \lambda y_n}
    \\
    &= \lambda \del{y_n + \frac{2}{3}h \lambda y_n}
    \\
    &= \lambda y_n \del{1 + \frac{2}{3}h \lambda}
    .
\end{align*}
%
Thus,
%
\begin{align*}
    y_{n + 1}
    &= y_n + h \del{\frac{1}{4} k_1 + \frac{3}{4} k_2}
    \\
    &= y_n + h \del{\frac{1}{4} \lambda y_n + \frac{3}{4} \lambda y_n \del{1 + \frac{2}{3}h \lambda}}
    \\
    &= \del{1 + h \lambda + \frac{1}{2} (h \lambda)^2} y_n
    \\
    &= \del{1 + h \lambda + \frac{1}{2} (h \lambda)^2}^n y_0
    .
\end{align*}

To satisfy $\lim_{n \to \infty} y_n = 0$ we need
%
\begin{equation*}
    \envert{1 + h \lambda + \frac{1}{2} (h \lambda)^2} < 1
    ,
\end{equation*}
%
which is the condition we were asked to show.

\vspace{5mm}

(b) Consider this inequality for the case of real $\lambda < 0$, and derive an upper stability bound for the step length $h$.

\textit{Solution.}
Now we assume that $\lambda < 0$, and use this to simplify the condition
as follows:
%
\begin{align*}
    &\envert{1 + h \lambda + \frac{1}{2} (h \lambda)^2} < 1
    \\
    &\iff -1 < 1 + h \lambda + \frac{1}{2} (h \lambda)^2 < 1
    \\
    &\iff -2 < h \lambda + \frac{1}{2} (h \lambda)^2 < 0
    .
\end{align*}
%
Note that
%
\begin{equation*}
    -2 < h \lambda + \frac{1}{2} (h \lambda)^2
\end{equation*}
%
is always satisfied for $h \lambda \in \R$.
Hence, since 
%
\begin{equation*}
    \lambda h + \frac{\lambda^2}{2} h^2 = 0
\end{equation*}
%
has solutions $h = 0, -2/\lambda$, if we want an upper stability bound for $h$
we see that this must be $-2/\lambda$.

\end{document}
